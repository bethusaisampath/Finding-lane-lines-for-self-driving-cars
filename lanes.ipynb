{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find Lanes for Self-Driving Cars using OpenCV python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![title](car.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading with all the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Loading image and converting to gray scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we convert the image to gray scale and have a look at it. Notice that i have created the copy of the image and performing any processing on that image copy. Since processing the images may reflect changes in the original image which we donâ€™t want. Converting to gray scale reduces the image dimension from 3(R,G,B) to single dimension. Detection of edges is much more accurate when done in gray scale images. Hence this conversion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Smoothing the gray scale image\n",
    "\n",
    "Once the image has been converted to gray scale, we then perform image smoothing. Images usually contain a lot of noise which make it difficult for algorithms to detect the edges. Smoothing the images does the detection task easier for us.\n",
    "You would be thinking what the (5,5) means in the first line of the code. Let me explain what image smoothing means.\n",
    "\n",
    "The Gaussian blur performs smoothing by averaging out the pixel intensity values through a weighted average kernel. (5,5) is the kernel size that is used to perform the smoothing operation. You are free to try out with different kernel size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Detecting edges using canny edge detection algorithm\n",
    "\n",
    "The purpose of edge detection in general is to significantly reduce the amount of data in an image, while preserving the structural properties to be used for further image processing.\n",
    "\n",
    "The algorithm runs in 5 separate steps:\n",
    "\n",
    "#### Smoothing: Blurring of the image to remove noise.\n",
    "#### Finding gradients: The edges should be marked where the gradients of the image has large magnitudes.\n",
    "#### Non-maximum suppression: Only local maxima should be marked as edges.\n",
    "#### Double thresholding : Potential edges are determined by thresholding.\n",
    "#### Edge tracking by hysteresis: Final edges are determined by suppressing all edges that are not connected to a very certain (strong) edge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#image=cv2.imread('test_image.jpg')\n",
    "#lane_image=np.copy(image)\n",
    "#canny_image=canny(lane_image)\n",
    "#cropped_image=region_of_interest(canny_image)\n",
    "\n",
    "def canny(image):\n",
    "    gray=cv2.cvtColor(image,cv2.COLOR_RGB2GRAY)\n",
    "    blur=cv2.GaussianBlur(gray,(5,5),0)\n",
    "    canny=cv2.Canny(blur,50,150)\n",
    "    return canny"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 : Creating masked image using region of interest\n",
    "\n",
    "Next we create an image mask which is a black image having same dimension as that of the original image. We then take the original image and identify the region of interest for us. In this case we have a triangle in place for the ROI which is then superimposed on the mask using opencv fillpoly() function and a region of interest mask is generated.\n",
    "\n",
    "### ROI mask\n",
    "Now we have the ROI mask and the canny image as well. The next step is to perform a bitwise and operator between both the images to get the Region of interest canny image.\n",
    "The bit wise and operator looks like 00000000 in the black section and 11111111 in the white section of the above image. When a bitwise_and operation is performed between the values and the values in the canny image the and operation with 00000000 will always generate a 00000000 that is a black pixel in the resulting image whereas a 11111111 will not have any effect on the resulting image and will generate a similar structure as the of the canny image in the white section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def region_of_interest(image):\n",
    "    height=image.shape[0]\n",
    "    polygons=np.array([[(200,height),(1100,height),(550,250)]])\n",
    "    mask=np.zeros_like(image)\n",
    "    cv2.fillPoly(mask,polygons,255)\n",
    "    masked_image=cv2.bitwise_and(image,mask)\n",
    "    return masked_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5 : Applying Hough transform on the image\n",
    "\n",
    "Before applying the hough transform method let us go through some of the theory behind it. The images in the image space is represented by the usual x and y axis as a 2D matrix of rows and columns that represents dimension of the image. The image in image space is represented by y = mx + b if we plot m and b as separate parameters along the x and y axis, this is called the parameter or hough space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![Image](1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![title](2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![Image](3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![Image](4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![Image](5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![Image](6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![Image](7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![Image](8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![Image](9.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Hough transform parameters\n",
    "# Make a blank the same size as our image to draw on\n",
    "#rho = 1 # distance resolution in pixels of the Hough grid\n",
    "#theta = np.pi/180 # angular resolution in radians of the Hough grid\n",
    "#threshold = 2     # minimum number of votes (intersections in Hough grid cell)\n",
    "#min_line_length = 40 #minimum number of pixels making up a line\n",
    "#max_line_gap = 5    # maximum gap in pixels between connectable line segments\n",
    "\n",
    "# Run Hough on edge detected image\n",
    "# Output \"lines\" is an array containing endpoints of detected line segments\n",
    "#lines = cv2.HoughLinesP(cropped_image, rho, theta, threshold, np.array([]), min_line_length, max_line_gap)\n",
    "#averaged_lines=average_slope_intercept(lane_image, lines)\n",
    "#line_image=display_lines(lane_image,averaged_lines)\n",
    "\n",
    "#combo_image = cv2.addWeighted(lane_image, 0.8, line_image, 1, 1)\n",
    "#cv2.imshow(\"result\",combo_image)\n",
    "#cv2.waitKey(0)\n",
    "#cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_coordinates(image,line_parameters):\n",
    "    (slope,intercept)=line_parameters\n",
    "    y1=image.shape[0]\n",
    "    y2=int(y1*(3/5))\n",
    "    x1=int((y1-intercept)/slope)\n",
    "    x2=int((y2-intercept)/slope)\n",
    "    return np.array([x1,y1,x2,y2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_slope_intercept(image, lines):\n",
    "    left_fit=[]\n",
    "    right_fit=[]\n",
    "    for line in lines:\n",
    "        x1,y1,x2,y2=line.reshape(4)\n",
    "        parameters=np.polyfit((x1,x2),(y1,y2),1)\n",
    "        slope=parameters[0]\n",
    "        intercept=parameters[1]\n",
    "        if slope<0:\n",
    "            left_fit.append((slope,intercept))\n",
    "        else:\n",
    "            right_fit.append((slope,intercept))\n",
    "    left_fit_average=np.average(left_fit,axis=0)\n",
    "    right_fit_average=np.average(right_fit,axis=0)\n",
    "    left_line=make_coordinates(image,left_fit_average)\n",
    "    right_line=make_coordinates(image,right_fit_average)\n",
    "    return np.array([left_line,right_line])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_lines(image,lines):\n",
    "    line_image=np.zeros_like(image)\n",
    "    if lines is not None:\n",
    "        for x1,y1,x2,y2 in lines:\n",
    "            cv2.line(line_image,(x1,y1),(x2,y2),(255,0,0),10)\n",
    "    return line_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final image\n",
    "Finally we are done with the image section. How you got an idea of how an image needs to be processed along with the computations happening in the background. Now we will extend this to videos. We will begin by importing the required packages for video analysis and processing in python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap=cv2.VideoCapture(\"test2.mp4\")\n",
    "while(cap.isOpened()):\n",
    "    _,frame=cap.read()\n",
    "    canny_image=canny(frame)\n",
    "    cropped_image=region_of_interest(canny_image)\n",
    "    # Define the Hough transform parameters\n",
    "    # Make a blank the same size as our image to draw on\n",
    "    rho = 1 # distance resolution in pixels of the Hough grid\n",
    "    theta = np.pi/180 # angular resolution in radians of the Hough grid\n",
    "    threshold = 2     # minimum number of votes (intersections in Hough grid cell)\n",
    "    min_line_length = 40 #minimum number of pixels making up a line\n",
    "    max_line_gap = 5    # maximum gap in pixels between connectable line segments\n",
    "\n",
    "    # Run Hough on edge detected image\n",
    "    # Output \"lines\" is an array containing endpoints of detected line segments\n",
    "    lines = cv2.HoughLinesP(cropped_image, rho, theta, threshold, np.array([]), min_line_length, max_line_gap)\n",
    "    averaged_lines=average_slope_intercept(frame, lines)\n",
    "    line_image=display_lines(frame,averaged_lines)\n",
    "    combo_image = cv2.addWeighted(frame, 0.8, line_image, 1, 1)\n",
    "    cv2.imshow(\"result\",combo_image)\n",
    "    if cv2.waitKey(1)==ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References:\n",
    "\n",
    "Self Driving Car Engineer Course | Udacity\n",
    "\n",
    "Build the future of transportation with Self Driving Car skillsin.udacity.com\n",
    "\n",
    "https://www.udemy.com/applied-deep-learningtm-the-complete-self-driving-car-course"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image of Yaktocat](https://octodex.github.com/images/yaktocat.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
